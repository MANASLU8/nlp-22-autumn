{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def to_tokens(fileName):\n",
    "    tsv = open(fileName).readlines()\n",
    "\n",
    "    items = [row.replace(\"\\n\", '').split('\\t')[0].lower() for row in tsv]\n",
    "    items = [x for x in items if re.match(r'[^\\\\<\\\\>\\\\:\\\\.\\\\,\\\\\"\\\\\\'\\\\$\\\\(\\\\)\\\\/\\\\\\\\-]+', x)]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    items = [w for w in items if not w.lower() in stop_words]\n",
    "    return items\n",
    "\n",
    "all_tokens = []\n",
    "for file in [x for x in os.listdir(\"dicts/\") if x.endswith('.tsv')]:\n",
    "    all_tokens += to_tokens(f\"dicts/{file}\")\n",
    "    \n",
    "print(all_tokens[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentences(fileName):\n",
    "    tsv = open(fileName).readlines()\n",
    "\n",
    "    items = [row.split('\\t')[0].lower() for row in tsv]\n",
    "    items = [x for x in items if re.match(r'[^\\\\<\\\\>\\\\:\\\\.\\\\,\\\\\"\\\\\\'\\\\$\\\\(\\\\)\\\\/\\\\\\\\-]+', x)]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    items = [w for w in items if not w.lower() in stop_words]\n",
    "    sentences = [[]]\n",
    "\n",
    "    for item in items:\n",
    "        if item == '\\n':\n",
    "            sentences.append([])\n",
    "        else:\n",
    "            sentences[-1].append(item)\n",
    "    return sentences\n",
    "\n",
    "all_sentences = []\n",
    "\n",
    "for file in [x for x in os.listdir(\"dicts/\") if x.endswith('.tsv')]:\n",
    "    all_sentences.append(to_sentences(f\"dicts/{file}\"))\n",
    "\n",
    "all_sentences = sum(all_sentences, [])\n",
    "\n",
    "print(all_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('all_sentences len: ', len(all_sentences))\n",
    "print('all_tokens len: ', len(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Считаем частоты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_map = {}\n",
    "for token in all_tokens:\n",
    "    frequency_map[token] = frequency_map.get(token, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаляем низкочастотные токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_map = {key:val for key, val in frequency_map.items() if val <= 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Строим матрицу Termin-Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "termin_document = pd.DataFrame(columns=frequency_map.keys())\n",
    "\n",
    "for folder in [x for x in os.listdir(\"dicts/\") if not x.endswith('.tsv')]:\n",
    "    for file in os.listdir(f'dicts/{folder}/'):\n",
    "        print(f\"{folder}/{file}\")\n",
    "        termin_document.loc[file] = 0\n",
    "        for token in to_tokens(f\"dicts/{folder}/\" + file):\n",
    "            try:\n",
    "                termin_document.at[f\"{folder}/{file}\", token] += 1\n",
    "            except:\n",
    "                # токена нет в укороченном словаре\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termin_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_frequency_map(tokens):\n",
    "    for token in tokens:\n",
    "        frequency_map[token] = frequency_map.get(token, 0) + 1\n",
    "    #frequency_map = {key:val for key, val in frequency_map.items() if val <= 6}\n",
    "    #return frequency_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_map = {}\n",
    "def vectorize(sentences):\n",
    "    #print(sentences)\n",
    "    #print(sum(sentences, []))\n",
    "    update_frequency_map(sum(sentences, []))\n",
    "    \n",
    "    freq_matrix = pd.DataFrame(columns=frequency_map.keys())\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            freq_matrix.loc[token] = 0\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            try:\n",
    "                freq_matrix.at[token, token] += 1\n",
    "            except:\n",
    "                pass\n",
    "    vector = []\n",
    "    for token in frequency_map:\n",
    "        vector.append(round(freq_matrix[token].mean(), 3))\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "count = 0\n",
    "for folder in [x for x in os.listdir(\"dicts/\") if not x.endswith('.tsv')]:\n",
    "    for file in os.listdir(f'dicts/{folder}/'):\n",
    "        count+=1\n",
    "        if count >= 101: break\n",
    "        print(f\"{folder}/{file}\")\n",
    "        tokens = to_sentences(f'dicts/{folder}/{file}')\n",
    "        vector = vectorize(tokens)\n",
    "        vectors.append(vector)\n",
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)\n",
    "pca_model = pca.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize(sentences):\n",
    "    freq_matrix = pd.DataFrame(columns=frequency_map.keys())\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            freq_matrix.loc[token] = 0\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            try:\n",
    "                freq_matrix.at[token, token] += 1\n",
    "            except:\n",
    "                pass\n",
    "    vector = []\n",
    "    for token in frequency_map:\n",
    "        vector.append(round(freq_matrix[token].mean(), 3))\n",
    "    vector = pca_model.transform(np.array(vector).reshape(1, -1))[0]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "def cosinus(a, b):\n",
    "    return 1 - spatial.distance.cosine(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, all_sentences):\n",
    "        self.all_sentences = all_sentences\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sentence in self.all_sentences:\n",
    "            yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim.models\n",
    "sentence_iterator = Train(all_sentences)\n",
    "model = gensim.models.Word2Vec(sentences=sentence_iterator)\n",
    "print('word2vec trained')\n",
    "model.save('models/word2vec_model')\n",
    "#print(model.wv.key_to_index)\n",
    "# check word2vec\n",
    "atheism = {'word': 'atheism', '1': ['evolution', 'heretic'], '2': ['humanism', 'agnostic'], '3': ['communism', 'university']}\n",
    "consistent = {'word': 'consistent', '1': ['compatible', 'agree'], '2': ['accordance', 'harmony'], '3': ['reconcile', 'control']}\n",
    "book = {'word': 'book', '1': ['volume', 'words'], '2': ['christian', 'churches'], '3': ['drums', 'guitar']}\n",
    "themes = [atheism, consistent, book]\n",
    "\n",
    "for index, word_dict in enumerate(themes):\n",
    "    vector = []\n",
    "    vect_main_word = model.wv[word_dict['word']].tolist()\n",
    "    vector.append(vect_main_word)\n",
    "    base_word = word_dict['word']\n",
    "    result = []\n",
    "    length = 2\n",
    "    for i in range(length):\n",
    "\n",
    "        similar_word = word_dict['1'][i]\n",
    "        field_word = word_dict['2'][i]\n",
    "        different_word = word_dict['3'][i]\n",
    "\n",
    "        vect_similar_word = model.wv[similar_word].tolist()\n",
    "        vect_field_word = model.wv[field_word].tolist()\n",
    "        vect_different_word = model.wv[different_word].tolist()\n",
    "        vector.extend([vect_similar_word, vect_field_word, vect_different_word])\n",
    "\n",
    "        value = cosinus(vect_main_word, vect_similar_word)\n",
    "        result.append((similar_word, value))\n",
    "        value = cosinus(vect_main_word, vect_field_word)\n",
    "        result.append((field_word, value))\n",
    "        value = cosinus(vect_main_word, vect_different_word)\n",
    "        result.append((different_word, value))\n",
    "\n",
    "    print(base_word, sorted(result, key=lambda a: a[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_vectorize(fileName):\n",
    "    model = gensim.models.Word2Vec.load('models/word2vec_model')\n",
    "    sentence_list = to_sentences(fileName)\n",
    "    vectors = []\n",
    "    for sentence in sentence_list:\n",
    "        for token in sentence:\n",
    "            try:\n",
    "                vectors.append(model.wv[token.lower()])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    v = np.zeros(model.vector_size)\n",
    "    v = (np.array([sum(x) for x in zip(*vectors)])) / v.size\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_text1_freq = vectorize(\"test/alt.atheism/53068.tsv\")\n",
    "vec_text1_w2v = w2v_vectorize(\"test/alt.atheism/53068.tsv\")\n",
    "vec_text2_freq = vectorize(\"dicts/alt.atheism/53313.tsv\")\n",
    "vec_text2_w2v = w2v_vectorize(\"dicts/alt.atheism/53313.tsv\")\n",
    "vec_text3_freq = vectorize(\"test/rec.autos/103007.tsv\")\n",
    "vec_text3_w2v = w2v_vectorize(\"test/rec.autos/103007.tsv\")\n",
    "freq_similarity = cosinus(vec_text1_freq, vec_text2_freq)\n",
    "word2vec_similarity = cosinus(vec_text1_w2v, vec_text2_w2v)\n",
    "freq_antisimilarity = cosinus(vec_text1_freq, vec_text3_freq)\n",
    "word2vec_antisimilarity = cosinus(vec_text1_w2v, vec_text3_w2v)\n",
    "print(f'Similarity for frequency vectorizer: {freq_similarity}')\n",
    "print(f'Similarity for word2vec vectorizer: {word2vec_similarity}')\n",
    "print(f'Antisimilarity for frequency vectorizer: {freq_antisimilarity}')\n",
    "print(f'Antisimilarity for word2vec vectorizer: {word2vec_antisimilarity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\"\n",
    "count = 0\n",
    "for folder in [x for x in os.listdir(\"test/\") if not x.endswith('.tsv')]:\n",
    "    for file in os.listdir(f'test/{folder}/'):\n",
    "        print(file, count)\n",
    "        count += 1\n",
    "        with open(f\"test/{folder}/{file}\") as fin:\n",
    "            vector = w2v_vectorize(f\"test/{folder}/{file}\")\n",
    "            row = \"\"\n",
    "            for embedding in vector:\n",
    "                row += '\\t' + str(round(embedding, 5))\n",
    "            result += (f\"{folder}/{file}\" + row + '\\n')\n",
    "with open('assets/annotated-corpus/test-embeddings.tsv', 'w') as result_file:\n",
    "    result_file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\"\n",
    "count = 0\n",
    "for folder in [x for x in os.listdir(\"train/\") if not x.endswith('.tsv')]:\n",
    "    for file in os.listdir(f'train/{folder}/'):\n",
    "        print(file, count)\n",
    "        count += 1\n",
    "        with open(f\"train/{folder}/{file}\") as fin:\n",
    "            vector = w2v_vectorize(f\"train/{folder}/{file}\")\n",
    "            row = \"\"\n",
    "            for embedding in vector:\n",
    "                row += '\\t' + str(round(embedding, 5))\n",
    "            result += (f\"{folder}/{file}\" + row + '\\n')\n",
    "with open('assets/annotated-corpus/train-embeddings.tsv', 'w') as result_file:\n",
    "    result_file.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed4d525ca6dc0f22f8b2fd4dc2dd129f85a6a249447cff82d5014df90417f5a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
