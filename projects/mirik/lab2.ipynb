{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from pprint import pprint\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|me|edu|ru|ua)\"\n",
    "digits = \"([0-9])\"\n",
    "word = \"([A-Za-z0-9][A-Za-z0-9]*)\"\n",
    "number = \"([1-9][0-9]*)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n>>>>\",\" \")\n",
    "    text = text.replace(\"\\n>>>\",\" \")\n",
    "    text = text.replace(\"\\n>>\",\" \")\n",
    "    text = text.replace(\"\\n>\",\" \")\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(f\"<{word}[.]{word}[.]{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<\\\\1<prd>\\\\2<prd>\\\\3<prd>\\\\4@\\\\5<prd>\\\\6<prd>\\\\7>\", text)\n",
    "    text = re.sub(f\"<{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<\\\\1<prd>\\\\2@\\\\3<prd>\\\\4<prd>\\\\5>\", text)\n",
    "    text = re.sub(f\"{word}@{word}[.]{word}[.]{word}\", \"\\\\1@\\\\2<prd>\\\\3<prd>\\\\4\", text)\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "def split_mail(text):\n",
    "    head_body = text.split(\"\\n\\n\")\n",
    "    return head_body[0], \"\\n\\n\".join(head_body[1:]) \n",
    "\n",
    "from re import S\n",
    "\n",
    "files = os.listdir(\"20news-bydate-train/alt.atheism/\")\n",
    "file = 0\n",
    "print(files[file])\n",
    "mail = open(\"20news-bydate-train/alt.atheism/\" + files[file], \"r\").read()\n",
    "head, body = split_mail(mail)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    #TODO: Можно еще смайлики добавить но мне влом их все перегонять в словарь\n",
    "    #TODO: Можно еще добавить время 22:11\n",
    "    sentence = re.sub(f\">{word}\", \"><split>\\\\1\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"\\\\({word}\", \"(<split>\\\\1\", sentence)\n",
    "    sentence = re.sub(f\"{word}\\\\)\", \"\\\\1<split>)\", sentence)\n",
    "    sentence = re.sub(f\"I'm\", \"I<split>am\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"${word}'d\", \"\\\\1<split>woud\", sentence)\n",
    "    sentence = re.sub(f\"{number},{number}\", \"\\\\1<coma>\\\\2\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{number},{number},{number}\", \"\\\\1<coma>\\\\2<coma>\\\\3\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{number}'{number},{number}\", \"\\\\1'\\\\2<coma>\\\\3\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{number}[.]{number}\", \"\\\\1<dot>\\\\2\", sentence)\n",
    "    sentence = re.sub(f\"{number}[.]{number}.{number}\", \"\\\\1<dot>\\\\2<dot>\\\\3\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{word}[.]{word}[.]{word}@{word}[.]{word}[.]{word}\", \"<split>\\\\1<dot>\\\\2<dot>\\\\3@\\\\4<dot>\\\\5<dot>\\\\6<split>\", sentence)\n",
    "    sentence = re.sub(f\"<{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<split>\\\\1<dot>\\\\2@\\\\3<dot>\\\\4<dot>\\\\5<split>\", sentence)\n",
    "    sentence = re.sub(f\"{word}@{word}[.]{word}[.]{word}\", \"\\\\1@\\\\2<dot>\\\\3<dot>\\\\4<split>\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"<{word}@{word}[.]{word}>\", \"<<split>\\\\1@\\\\2<dot>\\\\3<split>><split>\", sentence)\n",
    "    sentence = re.sub(f\"{word}@{word}[.]{word}\", \"\\\\1@\\\\2<dot>\\\\3<split>\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"<{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<split>\\\\1<dot>\\\\2@\\\\3<dot>\\\\4<dot>\\\\5<split>\", sentence)\n",
    "\n",
    "    sentence = sentence.replace(\"...\", '<dot><dot><dot>')\n",
    "    for r in \" \":\n",
    "        sentence = sentence.replace(r, '<split>')\n",
    "    for r in [\".\", \",\",\":\",\";\",\"?\",\"!\", '\"', \"'\", \"/\", \"*\", \"$\"]:\n",
    "        sentence = re.sub(f\"\\\\{r}{word}\", f\"{r}<split>\\\\1\", sentence)\n",
    "        sentence = re.sub(f\"{word}\\\\{r}\", f\"\\\\1<split>{r}\", sentence)\n",
    "\n",
    "    sentence = sentence.replace(\"<dot>\", \".\")\n",
    "\n",
    "    sentence = sentence.replace(\"<coma>\", \",\")\n",
    "\n",
    "    tokens = sentence.split('<split>')\n",
    "    return [(x, stemmer.stem(x), lemmatizer.lemmatize(stemmer.stem(x))) for x in tokens if x]\n",
    "\n",
    "# maxI = len(split_into_sentences(body)) - 1\n",
    "\n",
    "# @interact\n",
    "# def test(i=widgets.IntSlider(min=0,max=maxI,step=1,value=0)):\n",
    "#     sentence = split_into_sentences(body)[i]\n",
    "#     pprint(sentence)\n",
    "#     pprint(tokenize(sentence))\n",
    "\n",
    "# folders = os.listdir(\"20news-bydate-test/\")\n",
    "\n",
    "\n",
    "# for folder in folders:\n",
    "#     files = os.listdir(f\"20news-bydate-test/{folder}/\")\n",
    "#     print(folder)\n",
    "    \n",
    "#     for file in files:\n",
    "# # folder = \"alt.atheism\"\n",
    "# # file = \"49960\"\n",
    "\n",
    "#         if not os.path.exists(f\"out/{folder}\"):\n",
    "#             os.makedirs(f\"out/{folder}\")\n",
    "#         out = open(f\"out/{folder}/{file}.tsv\", \"w\")\n",
    "#         mail = open(f\"20news-bydate-test/{folder}/{file}\", \"r\").read()\n",
    "#         head, body = split_mail(mail)\n",
    "\n",
    "#         for sentence in split_into_sentences(head):\n",
    "#             for token, stem, lem in tokenize(sentence):\n",
    "#                 out.write(f\"{token}\\t{stem}\\t{lem}\\n\")\n",
    "#             out.write(\"\\n\")\n",
    "\n",
    "#         for sentence in split_into_sentences(body):\n",
    "#             for token, stem, lem in tokenize(sentence):\n",
    "#                 out.write(f\"{token}\\t{stem}\\t{lem}\\n\")\n",
    "#             out.write(\"\\n\")\n",
    "\n",
    "#         out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Моя жалкая попытка 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyboardArray = [\n",
    "    ['`','1','2','3','4','5','6','7','8','9','0','-','='],\n",
    "    ['q','w','e','r','t','y','u','i','o','p','[',']','\\\\'],\n",
    "    ['a','s','d','f','g','h','j','k','l',';','\\''],\n",
    "    ['z','x','c','v','b','n','m',',','.','/'],\n",
    "    ['', '', ' ', ' ', ' ', ' ', ' ', '', '']\n",
    "    ]\n",
    "\n",
    "shiftedKeyboardArray = [\n",
    "    ['~', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '+'],\n",
    "    ['Q', 'W', 'E', 'R', 'T', 'Y', 'U', 'I', 'O', 'P', '{', '}', '|'],\n",
    "    ['A', 'S', 'D', 'F', 'G', 'H', 'J', 'K', 'L', ':', '\"'],\n",
    "    ['Z', 'X', 'C', 'V', 'B', 'N', 'M', '<', '>', '?'],\n",
    "    ['', '', ' ', ' ', ' ', ' ', ' ', '', '']\n",
    "    ]\n",
    "\n",
    "def arrayForChar(c):\n",
    "    if (True in [c in r for r in keyboardArray]):\n",
    "        return keyboardArray\n",
    "    elif (True in [c in r for r in shiftedKeyboardArray]):\n",
    "        return shiftedKeyboardArray\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def getKeyboardXY(c, array):\n",
    "    if array == None:\n",
    "        return None\n",
    "    row = -1\n",
    "    column = -1\n",
    "    for r in array:\n",
    "        if c in r:\n",
    "            row = array.index(r)\n",
    "            column = r.index(c)\n",
    "            return (row, column)\n",
    "    return None\n",
    "\n",
    "def get_keyboard_distance(X, Y):\n",
    "    x = getKeyboardXY(X, arrayForChar(X))\n",
    "    y = getKeyboardXY(Y, arrayForChar(Y))\n",
    "    if x == None or y == None:\n",
    "        return 10\n",
    "    return ((x[0] - y[0])**2 + (x[1] - y[1])**2)**(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_removal_cost(X):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insertion_cost(X):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replacement_cost(X, Y):\n",
    "    return 1 #+ get_keyboard_distance(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:00\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def distance_vagner_fisher(x, y):\n",
    "    len_x, len_y = len(x), len(y)\n",
    "    if len_x > len_y:\n",
    "        x, y = y, x\n",
    "        len_x, len_y = len_y, len_x\n",
    "\n",
    "    current_column = range(len_x + 1)\n",
    "    for i in range(1, len_y + 1):\n",
    "        previous_column = current_column\n",
    "        current_column = [i] + [0] * len_x\n",
    "\n",
    "        for j in range(1, len_x + 1):\n",
    "            insert = previous_column[j] + get_insertion_cost(y[i - 1])\n",
    "            remove = current_column[j - 1] + get_removal_cost(x[j - 1])\n",
    "            replace = previous_column[j - 1]\n",
    "        \n",
    "            if x[j - 1] != y[i - 1]:\n",
    "                replace += get_replacement_cost(x[j - 1], y[i - 1])\n",
    "            # if replace > 8:\n",
    "            #     return None\n",
    "            current_column[j] = min(insert, remove, replace)\n",
    "\n",
    "    return current_column[len_x]\n",
    "\n",
    "a = 'hypothesis'\n",
    "b = 'reference'\n",
    "\n",
    "start_time = datetime.now()\n",
    "dist = distance_vagner_fisher(a, b)\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(dictionary, word):\n",
    "    if(len(word) <= 3):\n",
    "        return 0, word\n",
    "    dist_min, word_min = 10000, ''\n",
    "    for item in dictionary:\n",
    "        dist = distance_vagner_fisher(item, word)\n",
    "        if dist is not None and dist < dist_min:\n",
    "            dist_min = dist\n",
    "            word_min = item\n",
    "\n",
    "        if dist == 0 : break\n",
    "    \n",
    "    if dist_min == 10000:\n",
    "        return None\n",
    "\n",
    "    return dist_min, word_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From\tFrom\t0\n",
      ":\t:\t0\n",
      "decay@cbnewsj.cb.att\tdecay@cbnewsj.cb.att\t0\n",
      ".\t.\t0\n",
      "com\tcom\t0\n",
      "(\t(\t0\n",
      "dea\tdea\t0\n",
      ".\t.\t0\n",
      "kaflowitz\tkaflowitz\t0\n",
      ")\t)\t0\n",
      "Subject\tSubject\t0\n",
      ":\t:\t0\n",
      "Re\tRe\t0\n",
      ":\t:\t0\n",
      "aboot\tabout\t1\n",
      "the\tthe\t0\n",
      "ible\table\t1\n",
      "qyiz\tquit\t2\n",
      "answers\tanswers\t0\n",
      "Organizaion\torganization\t2\n",
      ":\t:\t0\n",
      "AT&T\tATF\t2\n",
      "Distribution\tdistribution\t1\n",
      ":\t:\t0\n",
      "na\tna\t0\n",
      "Linew\tnew\t2\n",
      ":\t:\t0\n",
      "18\t18\t0\n",
      "In\tIn\t0\n",
      "article\tarticle\t0\n",
      "<healta\t<healta\t0\n",
      ".\t.\t0\n",
      "153.735242337@saturn.wwc.edu\t56.734556346@saturn.wwc.edu>\t9\n",
      ">,\t>,\t0\n",
      "healta@saturn.wwc.ed\thealta@saturn.wwc.edu\t1\n",
      "(\t(\t0\n",
      "Tammy\tTammy\t0\n",
      "R\tR\t0\n",
      "Heali\tHealy\t1\n",
      ")\t)\t0\n",
      "writes\twrites\t0\n",
      ":\t:\t0\n",
      "#12\t#12\t0\n",
      ")\t)\t0\n",
      "The\tThe\t0\n",
      "2\t2\t0\n",
      "cheribums\tserious\t4\n",
      "ae\tae\t0\n",
      "on\ton\t0\n",
      "the\tthe\t0\n",
      "Ark\tArk\t0\n",
      "of\tof\t0\n",
      "the\tthe\t0\n",
      "Cobenant\tcovenant\t2\n",
      ".\t.\t0\n",
      "When\tWhen\t0\n",
      "God\tGod\t0\n",
      "saod\tsod\t1\n",
      "msk\tmsk\t0\n",
      "no\tno\t0\n",
      "raven\thaven\t1\n",
      "imae\timam\t1\n",
      ",\t,\t0\n",
      "he\the\t0\n",
      "was\twas\t0\n",
      "referin\trefering\t1\n",
      "to\tto\t0\n",
      "idoms\tidols\t1\n",
      ",\t,\t0\n",
      "which\twhich\t0\n",
      "were\twere\t0\n",
      "created\tcreated\t0\n",
      "to\tto\t0\n",
      "be\tbe\t0\n",
      "worsipped\tworshipped\t1\n",
      ".\t.\t0\n",
      "The\tThe\t0\n",
      "Ark\tArk\t0\n",
      "of\tof\t0\n",
      "the\tthe\t0\n",
      "Covenant\tcovenant\t1\n",
      "wawn\twasn\t1\n",
      "'\t'\t0\n",
      "t\tt\t0\n",
      "wrodipped\tripped\t3\n",
      "znd\tznd\t0\n",
      "only\tonly\t0\n",
      "the\tthe\t0\n",
      "high\thigh\t0\n",
      "prmest\tpriest\t1\n",
      "could\tcould\t0\n",
      "nter\tenter\t1\n",
      "the\tthe\t0\n",
      "Holy\tHoly\t0\n",
      "of\tof\t0\n",
      "Holies\tlies\t2\n",
      "where\twhere\t0\n",
      "it\tit\t0\n",
      "was\twas\t0\n",
      "kept\tkept\t0\n",
      "once\tonce\t0\n",
      "a\ta\t0\n",
      "year\tyear\t0\n",
      ",\t,\t0\n",
      "on\ton\t0\n",
      "the\tthe\t0\n",
      "Day\tDay\t0\n",
      "of\tof\t0\n",
      "Atonement\tatonement\t1\n",
      ".\t.\t0\n",
      "I\tI\t0\n",
      "zm\tzm\t0\n",
      "not\tnot\t0\n",
      "famklir\tfamiliar\t2\n",
      "with\twith\t0\n",
      ",\t,\t0\n",
      "or\tor\t0\n",
      "knowledgeable\tknowledgeable\t0\n",
      "about\tabout\t0\n",
      "the\tthe\t0\n",
      "orijia\torigin\t2\n",
      "languge\tlanguage\t1\n",
      ",\t,\t0\n",
      "but\tbut\t0\n",
      "I\tI\t0\n",
      "btlieve\tbelieve\t1\n",
      "thre\tthe\t1\n",
      "is\tis\t0\n",
      "a\ta\t0\n",
      "word\tword\t0\n",
      "for\tfor\t0\n",
      "\"\t\"\t0\n",
      "idol\tidols\t1\n",
      "\"\t\"\t0\n",
      "and\tand\t0\n",
      "that\tthat\t0\n",
      "the\tthe\t0\n",
      "ranslator\ttranslator\t1\n",
      "would\twould\t0\n",
      "have\thave\t0\n",
      "used\tused\t0\n",
      "the\tthe\t0\n",
      "wor\twor\t0\n",
      "\"\t\"\t0\n",
      "ol\tol\t0\n",
      "\"\t\"\t0\n",
      "insteqd\tinstead\t1\n",
      "of\tof\t0\n",
      "\"\t\"\t0\n",
      "graven\tgrave\t1\n",
      "imge\timage\t1\n",
      "\"\t\"\t0\n",
      "had\thad\t0\n",
      "the\tthe\t0\n",
      "orignal\toriginal\t1\n",
      "said\tsaid\t0\n",
      "\"\t\"\t0\n",
      "dol\tdol\t0\n",
      "\".\t\".\t0\n",
      "So\tSo\t0\n",
      "I\tI\t0\n",
      "think\tthink\t0\n",
      "you\tyou\t0\n",
      "'\t'\t0\n",
      "re\tre\t0\n",
      "wrong\twrong\t0\n",
      "hqre\there\t1\n",
      ",\t,\t0\n",
      "bt\tbt\t0\n",
      "then\tthen\t0\n",
      "abain\tagain\t1\n",
      "I\tI\t0\n",
      "could\tcould\t0\n",
      "to\tto\t0\n",
      ".\t.\t0\n",
      "I\tI\t0\n",
      "just\tjust\t0\n",
      "suggcstig\tsuggest\t3\n",
      "a\ta\t0\n",
      "wa\twa\t0\n",
      "o\to\t0\n",
      "determine\tdetermine\t0\n",
      "whether\twhether\t0\n",
      "the\tthe\t0\n",
      "ynterpregation\tinterpretation\t2\n",
      "you\tyou\t0\n",
      "offer\toffer\t0\n",
      "os\tos\t0\n",
      "correct\tcorrect\t0\n",
      ".\t.\t0\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir(\"test_data/\"):\n",
    "    files = os.listdir(f\"test_data/{folder}/\")\n",
    "    for file in files:\n",
    "        with open(f\"out/{file}\", \"w\") as fout:\n",
    "            with open(f'dicts/{folder}.tsv') as dict_file:\n",
    "                dictionary = csv.reader(dict_file, delimiter=\"\\t\")\n",
    "                dictionary = [x[0] for x in dictionary if len(x[0]) > 2]\n",
    "                dictionary.sort(key = lambda x:len(x))\n",
    "                content = open(f\"test_data/{folder}/{file}\", \"r\").read()\n",
    "                for sentence in split_into_sentences(content):\n",
    "                    for token, stem, lem in tokenize(sentence):\n",
    "                        matched =  match(dictionary, token)\n",
    "                        if matched is not None:\n",
    "                            print(f\"{token}\\t{matched[1]}\\t{matched[0]}\")\n",
    "                            fout.write(f\"{token}\\t{matched[1]}\\t{matched[0]}\\n\")\n",
    "                        else:\n",
    "                            print(f\"{token}\\t{token}\")\n",
    "                            fout.write(f\"{token}\\t{token}\\t0\\n\")\n",
    "                    fout.write(\"\\n\")\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ander\\Desktop\\NLP\\lab2\\lab1.ipynb Ячейка 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ander/Desktop/NLP/lab2/lab1.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m tsv:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ander/Desktop/NLP/lab2/lab1.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     t \u001b[39m=\u001b[39m row\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ander/Desktop/NLP/lab2/lab1.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m t[\u001b[39m2\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ander/Desktop/NLP/lab2/lab1.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         non_replaced \u001b[39m=\u001b[39m non_replaced \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ander/Desktop/NLP/lab2/lab1.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(non_replaced\u001b[39m/\u001b[39mtotal)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"out/\"):\n",
    "    tsv = open(f\"out/{file}\").readlines()\n",
    "    total = len(tsv)\n",
    "    non_replaced = 0\n",
    "    for row in tsv:\n",
    "        t = row.replace(\"\\n\", '').split('\\t')\n",
    "        if t[2] == '0':\n",
    "            non_replaced = non_replaced + 1\n",
    "    print(non_replaced/total)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "original = [[]]\n",
    "replaced = [[]]\n",
    "before_replaced = [[]]\n",
    "\n",
    "with open(\"out/53068\") as f_replaced:\n",
    "    with open(\"lab1_tokens/53068.tsv\") as f_original:\n",
    "        for a_origibal in f_original.readlines():\n",
    "            if a_origibal == \"\\n\":\n",
    "                original.append([])\n",
    "                continue\n",
    "            a_origibal = a_origibal.replace(\"\\n\", \"\").split('\\t')\n",
    "            original[-1].append(a_origibal[0].lower())\n",
    "        \n",
    "        for b_replaced in f_replaced.readlines():\n",
    "            if b_replaced == \"\\n\":\n",
    "                replaced.append([])\n",
    "                before_replaced.append([])\n",
    "                continue\n",
    "            b_replaced = b_replaced.replace(\"\\n\", \"\").split('\\t')\n",
    "            replaced[-1].append(b_replaced[1].lower())\n",
    "            before_replaced[-1].append(b_replaced[0].lower())\n",
    "        \n",
    "        # for word in original:\n",
    "        #     total = total + 1\n",
    "        #     if word in replaced:\n",
    "        #         r = r + 1\n",
    "\n",
    "# print(total)\n",
    "# print(r)\n",
    "# print(r/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "129\n",
      "0.86\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "r = 0\n",
    "\n",
    "for sentence in original:\n",
    "    for replaced_sentence in replaced:\n",
    "        if len(sentence) == len(replaced_sentence):\n",
    "            for word in sentence:\n",
    "                total = total + 1\n",
    "                if word in replaced_sentence:\n",
    "                    r = r + 1\n",
    "print(total)\n",
    "print(r)\n",
    "print(r/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "118\n",
      "0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "r = 0\n",
    "\n",
    "for sentence in original:\n",
    "    for replaced_sentence in before_replaced:\n",
    "        if len(sentence) == len(replaced_sentence):\n",
    "            for word in sentence:\n",
    "                total = total + 1\n",
    "                if word in replaced_sentence:\n",
    "                    r = r + 1\n",
    "print(total)\n",
    "print(r)\n",
    "print(r/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['from', ':', 'decay@cbnewsj.cb.att', '.', 'com', '(', 'dean', '.'],\n",
       " ['in', 'article', '<healta', '.'],\n",
       " ['153.735242337@saturn.wwc.edu',\n",
       "  '>,',\n",
       "  'healta@saturn.wwc.edu',\n",
       "  '(',\n",
       "  'tammy',\n",
       "  'r',\n",
       "  'healy',\n",
       "  ')',\n",
       "  'writes',\n",
       "  ':',\n",
       "  '#12',\n",
       "  ')',\n",
       "  'the',\n",
       "  '2',\n",
       "  'cheribums',\n",
       "  'are',\n",
       "  'on',\n",
       "  'the',\n",
       "  'ark',\n",
       "  'of',\n",
       "  'the',\n",
       "  'covenant',\n",
       "  '.'],\n",
       " ['when',\n",
       "  'god',\n",
       "  'said',\n",
       "  'make',\n",
       "  'no',\n",
       "  'graven',\n",
       "  'image',\n",
       "  ',',\n",
       "  'he',\n",
       "  'was',\n",
       "  'refering',\n",
       "  'to',\n",
       "  'idols',\n",
       "  ',',\n",
       "  'which',\n",
       "  'were',\n",
       "  'created',\n",
       "  'to',\n",
       "  'be',\n",
       "  'worshipped',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'ark',\n",
       "  'of',\n",
       "  'the',\n",
       "  'covenant',\n",
       "  'wasn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'wrodhipped',\n",
       "  'and',\n",
       "  'only',\n",
       "  'the',\n",
       "  'high',\n",
       "  'priest',\n",
       "  'could',\n",
       "  'enter',\n",
       "  'the',\n",
       "  'holy',\n",
       "  'of',\n",
       "  'holies',\n",
       "  'where',\n",
       "  'it',\n",
       "  'was',\n",
       "  'kept',\n",
       "  'once',\n",
       "  'a',\n",
       "  'year',\n",
       "  ',',\n",
       "  'on',\n",
       "  'the',\n",
       "  'day',\n",
       "  'of',\n",
       "  'atonement',\n",
       "  '.'],\n",
       " ['i',\n",
       "  'am',\n",
       "  'not',\n",
       "  'familiar',\n",
       "  'with',\n",
       "  ',',\n",
       "  'or',\n",
       "  'knowledgeable',\n",
       "  'about',\n",
       "  'the',\n",
       "  'original',\n",
       "  'language',\n",
       "  ',',\n",
       "  'but',\n",
       "  'i',\n",
       "  'believe',\n",
       "  'there',\n",
       "  'is',\n",
       "  'a',\n",
       "  'word',\n",
       "  'for',\n",
       "  '\"',\n",
       "  'idol',\n",
       "  '\"',\n",
       "  'and',\n",
       "  'that',\n",
       "  'the',\n",
       "  'translator',\n",
       "  'would',\n",
       "  'have',\n",
       "  'used',\n",
       "  'the',\n",
       "  'word',\n",
       "  '\"',\n",
       "  'idol',\n",
       "  '\"',\n",
       "  'instead',\n",
       "  'of',\n",
       "  '\"',\n",
       "  'graven',\n",
       "  'image',\n",
       "  '\"',\n",
       "  'had',\n",
       "  'the',\n",
       "  'original',\n",
       "  'said',\n",
       "  '\"',\n",
       "  'idol',\n",
       "  '\".'],\n",
       " ['so',\n",
       "  'i',\n",
       "  'think',\n",
       "  'you',\n",
       "  \"'\",\n",
       "  're',\n",
       "  'wrong',\n",
       "  'here',\n",
       "  ',',\n",
       "  'but',\n",
       "  'then',\n",
       "  'again',\n",
       "  'i',\n",
       "  'could',\n",
       "  'be',\n",
       "  'too',\n",
       "  '.'],\n",
       " ['i',\n",
       "  'just',\n",
       "  'suggesting',\n",
       "  'a',\n",
       "  'way',\n",
       "  'to',\n",
       "  'determine',\n",
       "  'whether',\n",
       "  'the',\n",
       "  'interpretation',\n",
       "  'you',\n",
       "  'offer',\n",
       "  'is',\n",
       "  'correct',\n",
       "  '.'],\n",
       " []]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['from', ':', 'decay@cbnewsj.cb.att', '.', 'com', '(', 'dea', '.'],\n",
       " ['kaflowitz',\n",
       "  ')',\n",
       "  'subject',\n",
       "  ':',\n",
       "  're',\n",
       "  ':',\n",
       "  'about',\n",
       "  'the',\n",
       "  'able',\n",
       "  'quit',\n",
       "  'answers',\n",
       "  'organization',\n",
       "  ':',\n",
       "  'atf',\n",
       "  'distribution',\n",
       "  ':',\n",
       "  'na',\n",
       "  'new',\n",
       "  ':',\n",
       "  '18',\n",
       "  'in',\n",
       "  'article',\n",
       "  '<healta',\n",
       "  '.'],\n",
       " ['56.734556346@saturn.wwc.edu>',\n",
       "  '>,',\n",
       "  'healta@saturn.wwc.edu',\n",
       "  '(',\n",
       "  'tammy',\n",
       "  'r',\n",
       "  'healy',\n",
       "  ')',\n",
       "  'writes',\n",
       "  ':',\n",
       "  '#12',\n",
       "  ')',\n",
       "  'the',\n",
       "  '2',\n",
       "  'serious',\n",
       "  'ae',\n",
       "  'on',\n",
       "  'the',\n",
       "  'ark',\n",
       "  'of',\n",
       "  'the',\n",
       "  'covenant',\n",
       "  '.'],\n",
       " ['when',\n",
       "  'god',\n",
       "  'sod',\n",
       "  'msk',\n",
       "  'no',\n",
       "  'haven',\n",
       "  'imam',\n",
       "  ',',\n",
       "  'he',\n",
       "  'was',\n",
       "  'refering',\n",
       "  'to',\n",
       "  'idols',\n",
       "  ',',\n",
       "  'which',\n",
       "  'were',\n",
       "  'created',\n",
       "  'to',\n",
       "  'be',\n",
       "  'worshipped',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'ark',\n",
       "  'of',\n",
       "  'the',\n",
       "  'covenant',\n",
       "  'wasn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'ripped',\n",
       "  'znd',\n",
       "  'only',\n",
       "  'the',\n",
       "  'high',\n",
       "  'priest',\n",
       "  'could',\n",
       "  'enter',\n",
       "  'the',\n",
       "  'holy',\n",
       "  'of',\n",
       "  'lies',\n",
       "  'where',\n",
       "  'it',\n",
       "  'was',\n",
       "  'kept',\n",
       "  'once',\n",
       "  'a',\n",
       "  'year',\n",
       "  ',',\n",
       "  'on',\n",
       "  'the',\n",
       "  'day',\n",
       "  'of',\n",
       "  'atonement',\n",
       "  '.'],\n",
       " ['i',\n",
       "  'zm',\n",
       "  'not',\n",
       "  'familiar',\n",
       "  'with',\n",
       "  ',',\n",
       "  'or',\n",
       "  'knowledgeable',\n",
       "  'about',\n",
       "  'the',\n",
       "  'origin',\n",
       "  'language',\n",
       "  ',',\n",
       "  'but',\n",
       "  'i',\n",
       "  'believe',\n",
       "  'the',\n",
       "  'is',\n",
       "  'a',\n",
       "  'word',\n",
       "  'for',\n",
       "  '\"',\n",
       "  'idols',\n",
       "  '\"',\n",
       "  'and',\n",
       "  'that',\n",
       "  'the',\n",
       "  'translator',\n",
       "  'would',\n",
       "  'have',\n",
       "  'used',\n",
       "  'the',\n",
       "  'wor',\n",
       "  '\"',\n",
       "  'ol',\n",
       "  '\"',\n",
       "  'instead',\n",
       "  'of',\n",
       "  '\"',\n",
       "  'grave',\n",
       "  'image',\n",
       "  '\"',\n",
       "  'had',\n",
       "  'the',\n",
       "  'original',\n",
       "  'said',\n",
       "  '\"',\n",
       "  'dol',\n",
       "  '\".'],\n",
       " ['so',\n",
       "  'i',\n",
       "  'think',\n",
       "  'you',\n",
       "  \"'\",\n",
       "  're',\n",
       "  'wrong',\n",
       "  'here',\n",
       "  ',',\n",
       "  'bt',\n",
       "  'then',\n",
       "  'again',\n",
       "  'i',\n",
       "  'could',\n",
       "  'to',\n",
       "  '.'],\n",
       " ['i',\n",
       "  'just',\n",
       "  'suggest',\n",
       "  'a',\n",
       "  'wa',\n",
       "  'o',\n",
       "  'determine',\n",
       "  'whether',\n",
       "  'the',\n",
       "  'interpretation',\n",
       "  'you',\n",
       "  'offer',\n",
       "  'os',\n",
       "  'correct',\n",
       "  '.'],\n",
       " []]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "239a9fa0287fc0fb48e1b84671738ced5172aadbf780861bac62c840610302f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
