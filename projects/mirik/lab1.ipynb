{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from pprint import pprint\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скачиваем wordnet и omw\n",
    "\n",
    "`Wordnet` нужен чтобы на основе него строить леммы\n",
    "\n",
    "`omw` нужен чтобы делать стеммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регулярки\n",
    "\n",
    "Эти регулярки используются для разбиения по сентензам или по токенам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|me|edu|ru|ua)\"\n",
    "digits = \"([0-9])\"\n",
    "word = \"([A-Za-z0-9][A-Za-z0-9]*)\"\n",
    "number = \"([1-9][0-9]*)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бьём на сентензы (утверждения)\n",
    "\n",
    "Суть в том чтобы разбить по условным знакам окончания предложения разбить на эти самые предложения.\n",
    "\n",
    "Внутри предложения могут быть другие точки, прим: адрес электронной почты. Эти точки нужно экранировать например как `<dot>` и в самом конце сделать `replace` `text` по `<dot>`\n",
    "У меня в коде `<dot>` это `<prd>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n>>>>\",\" \")\n",
    "    text = text.replace(\"\\n>>>\",\" \")\n",
    "    text = text.replace(\"\\n>>\",\" \")\n",
    "    text = text.replace(\"\\n>\",\" \")\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(f\"<{word}[.]{word}[.]{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<\\\\1<prd>\\\\2<prd>\\\\3<prd>\\\\4@\\\\5<prd>\\\\6<prd>\\\\7>\", text)\n",
    "    text = re.sub(f\"<{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<\\\\1<prd>\\\\2@\\\\3<prd>\\\\4<prd>\\\\5>\", text)\n",
    "    text = re.sub(f\"{word}@{word}[.]{word}[.]{word}\", \"\\\\1@\\\\2<prd>\\\\3<prd>\\\\4\", text)\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "split_into_sentences(\"\"\"\n",
    "In article <1993Mar25.161909.8110@wuecl.wustl.edu> dp@cec1.wustl.edu (David Prutchi) writes:\n",
    ">In article <C4CntG.Jv4@spk.hp.com> long@spk.hp.com (Jerry Long) writes:\n",
    ">>Fred W. Culpepper (fculpepp@norfolk.vak12ed.edu) wrote:\n",
    ">>[...]\n",
    ">>A couple of years ago I put together a Tesla circuit which\n",
    ">>was published in an electronics magazine and could have been\n",
    ">>the circuit which is referred to here. This one used a\n",
    ">>flyback transformer from a tv onto which you wound your own\n",
    ">>primary windings. It also used 2 power transistors in a TO 3\n",
    ">[...]\n",
    ">10 years ago I built a 1'000,000 volt Tesla, and the thing was VERY\n",
    ">spectacular, but besides scaring/amazing friends (depending on their\n",
    ">knowledge of Science), and generating strong EMI, I never found anything\n",
    ">useful that could be done with it ...  Is there any real-world application\n",
    ">for Tesla coils today ?\n",
    ">\n",
    ">David Prutchi\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция для сплита письма на заголовок и тело письма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mail(text):\n",
    "    head_body = text.split(\"\\n\\n\")\n",
    "    return head_body[0], \"\\n\\n\".join(head_body[1:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример письма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mail = open(\"20news-bydate-train/sci.electronics/52434\", \"r\").read()\n",
    "print(mail)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат сплита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head, body = split_mail(mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = split_into_sentences(body)\n",
    "# for item in items:\n",
    "#     print(item)\n",
    "\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бьём на токены\n",
    "\n",
    "`tokenize` принимает на вход предложение и внутри него ищет токены.\n",
    "\n",
    "Снова Сначала ищем email и т п, реплейсим все точки запятые на `<dot>` `<coma>`\n",
    "\n",
    "\n",
    "После того как их зареплейсили, можно бить по точкам, запятым и прочему мусору.\n",
    "\n",
    "После того как побили токен, внутри него реплейсим `<dot>` обратно на точку, `<coma> обратно на запятую`\n",
    "\n",
    "В конце функции считаем стемму по токену, по стемме считаем лемму\n",
    "\n",
    "Интеракт функция - слайдер чтобы можно было листать предложения и смотреть результат разбиения в интеративе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import S\n",
    "\n",
    "\n",
    "files = os.listdir(\"20news-bydate-train/alt.atheism/\")\n",
    "file = 0\n",
    "print(files[file])\n",
    "mail = open(\"20news-bydate-train/alt.atheism/\" + files[file], \"r\").read()\n",
    "head, body = split_mail(mail)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    #TODO: Можно еще смайлики добавить но мне влом их все перегонять в словарь\n",
    "    #TODO: Можно еще добавить время 22:11\n",
    "    sentence = re.sub(f\">{word}\", \"><split>\\\\1\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"\\\\({word}\", \"(<split>\\\\1\", sentence)\n",
    "    sentence = re.sub(f\"{word}\\\\)\", \"\\\\1<split>)\", sentence)\n",
    "    sentence = re.sub(f\"I'm\", \"I<split>am\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"${word}'d\", \"\\\\1<split>woud\", sentence)\n",
    "    sentence = re.sub(f\"{number},{number}\", \"\\\\1<coma>\\\\2\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{number},{number},{number}\", \"\\\\1<coma>\\\\2<coma>\\\\3\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{number}'{number},{number}\", \"\\\\1'\\\\2<coma>\\\\3\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{number}[.]{number}\", \"\\\\1<dot>\\\\2\", sentence)\n",
    "    sentence = re.sub(f\"{number}[.]{number}.{number}\", \"\\\\1<dot>\\\\2<dot>\\\\3\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{word}[.]{word}[.]{word}@{word}[.]{word}[.]{word}\", \"<split>\\\\1<dot>\\\\2<dot>\\\\3@\\\\4<dot>\\\\5<dot>\\\\6<split>\", sentence)\n",
    "    sentence = re.sub(f\"<{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<split>\\\\1<dot>\\\\2@\\\\3<dot>\\\\4<dot>\\\\5<split>\", sentence)\n",
    "    sentence = re.sub(f\"{word}@{word}[.]{word}[.]{word}\", \"\\\\1@\\\\2<dot>\\\\3<dot>\\\\4<split>\", sentence)\n",
    "    sentence = re.sub(f\"<{word}@{word}[.]{word}>\", \"<<split>\\\\1@\\\\2<dot>\\\\3<split>><split>\", sentence)\n",
    "    sentence = re.sub(f\"{word}@{word}[.]{word}\", \"\\\\1@\\\\2<dot>\\\\3<split>\", sentence)\n",
    "    sentence = re.sub(f\"<{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<split>\\\\1<dot>\\\\2@\\\\3<dot>\\\\4<dot>\\\\5<split>\", sentence)\n",
    "\n",
    "    sentence = sentence.replace(\"...\", '<dot><dot><dot>')\n",
    "    for r in \" \":\n",
    "        sentence = sentence.replace(r, '<split>')\n",
    "    for r in [\".\", \",\",\":\",\";\",\"?\",\"!\", '\"', \"'\", \"/\", \"*\", \"$\"]:\n",
    "        sentence = re.sub(f\"\\\\{r}{word}\", f\"{r}<split>\\\\1\", sentence)\n",
    "        sentence = re.sub(f\"{word}\\\\{r}\", f\"\\\\1<split>{r}\", sentence)\n",
    "\n",
    "    sentence = sentence.replace(\"<dot>\", \".\")\n",
    "\n",
    "    sentence = sentence.replace(\"<coma>\", \",\")\n",
    "\n",
    "    tokens = sentence.split('<split>')\n",
    "    return [(x, stemmer.stem(x), lemmatizer.lemmatize(stemmer.stem(x))) for x in tokens if x]\n",
    "\n",
    "maxI = len(split_into_sentences(body)) - 1\n",
    "\n",
    "@interact\n",
    "def test(i=widgets.IntSlider(min=0,max=maxI,step=1,value=0)):\n",
    "    sentence = split_into_sentences(body)[i]\n",
    "    pprint(sentence)\n",
    "    pprint(tokenize(sentence))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бьём на токены все файлы в директории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir(\"20news-bydate-train/\")\n",
    "print(folders)\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(f\"20news-bydate-train/{folder}/\")\n",
    "    \n",
    "    print(folder)\n",
    "    \n",
    "    for file in files:\n",
    "        if not os.path.exists(f\"out/train/{folder}\"):\n",
    "            os.makedirs(f\"out/train/{folder}\")\n",
    "        out = open(f\"out/train/{folder}/{file}.tsv\", \"w\")\n",
    "\n",
    "        mail = open(f\"20news-bydate-train/{folder}/{file}\", \"r\").read()\n",
    "        head, body = split_mail(mail)\n",
    "\n",
    "        for sentence in split_into_sentences(head):\n",
    "            for token, stem, lem in tokenize(sentence):\n",
    "                out.write(f\"{token}\\t{stem}\\t{lem}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "        for sentence in split_into_sentences(body):\n",
    "            for token, stem, lem in tokenize(sentence):\n",
    "                out.write(f\"{token}\\t{stem}\\t{lem}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir(\"20news-bydate-train/\")\n",
    "print(folders)\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(f\"20news-bydate-train/{folder}/\")\n",
    "    \n",
    "    print(folder)\n",
    "    out = open(f\"out/train/{folder}.tsv\", \"w\")\n",
    "    for file in files:\n",
    "        if not os.path.exists(f\"out/train/{folder}\"):\n",
    "            os.makedirs(f\"out/train/{folder}\")\n",
    "        \n",
    "\n",
    "        mail = open(f\"20news-bydate-train/{folder}/{file}\", \"r\").read()\n",
    "        head, body = split_mail(mail)\n",
    "\n",
    "        for sentence in split_into_sentences(head):\n",
    "            for token, stem, lem in tokenize(sentence):\n",
    "                out.write(f\"{token}\\t{stem}\\t{lem}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "        for sentence in split_into_sentences(body):\n",
    "            for token, stem, lem in tokenize(sentence):\n",
    "                out.write(f\"{token}\\t{stem}\\t{lem}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "    out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "239a9fa0287fc0fb48e1b84671738ced5172aadbf780861bac62c840610302f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
