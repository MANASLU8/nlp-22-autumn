{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from pprint import pprint\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скачиваем wordnet и omw\n",
    "\n",
    "`Wordnet` нужен чтобы на основе него строить леммы\n",
    "\n",
    "`omw` нужен чтобы делать стеммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регулярки\n",
    "\n",
    "Эти регулярки используются для разбиения по сентензам или по токенам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|me|edu|ru|ua)\"\n",
    "digits = \"([0-9])\"\n",
    "word = \"([A-Za-z0-9][A-Za-z0-9]*)\"\n",
    "number = \"([1-9][0-9]*)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бьём на сентензы (утверждения)\n",
    "\n",
    "Суть в том чтобы разбить по условным знакам окончания предложения разбить на эти самые предложения.\n",
    "\n",
    "Внутри предложения могут быть другие точки, прим: адрес электронной почты. Эти точки нужно экранировать например как `<dot>` и в самом конце сделать `replace` `text` по `<dot>`\n",
    "У меня в коде `<dot>` это `<prd>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In article <1993Mar25.161909.8110@wuecl.wustl.edu> dp@cec1.wustl.edu (David Prutchi) writes: In article <C4CntG.Jv4@spk.hp.com> long@spk.hp.com (Jerry Long) writes: Fred W. Culpepper (fculpepp@norfolk.vak12ed.edu) wrote: [...] A couple of years ago I put together a Tesla circuit which was published in an electronics magazine and could have been the circuit which is referred to here.',\n",
       " 'This one used a flyback transformer from a tv onto which you wound your own primary windings.',\n",
       " \"It also used 2 power transistors in a TO 3 [...] 10 years ago I built a 1'000,000 volt Tesla, and the thing was VERY spectacular, but besides scaring/amazing friends (depending on their knowledge of Science), and generating strong EMI, I never found anything useful that could be done with it ...  Is there any real-world application for Tesla coils today ?\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n>>>>\",\" \")\n",
    "    text = text.replace(\"\\n>>>\",\" \")\n",
    "    text = text.replace(\"\\n>>\",\" \")\n",
    "    text = text.replace(\"\\n>\",\" \")\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(f\"<{word}[.]{word}[.]{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<\\\\1<prd>\\\\2<prd>\\\\3<prd>\\\\4@\\\\5<prd>\\\\6<prd>\\\\7>\", text)\n",
    "    text = re.sub(f\"<{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<\\\\1<prd>\\\\2@\\\\3<prd>\\\\4<prd>\\\\5>\", text)\n",
    "    text = re.sub(f\"{word}@{word}[.]{word}[.]{word}\", \"\\\\1@\\\\2<prd>\\\\3<prd>\\\\4\", text)\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "split_into_sentences(\"\"\"\n",
    "In article <1993Mar25.161909.8110@wuecl.wustl.edu> dp@cec1.wustl.edu (David Prutchi) writes:\n",
    ">In article <C4CntG.Jv4@spk.hp.com> long@spk.hp.com (Jerry Long) writes:\n",
    ">>Fred W. Culpepper (fculpepp@norfolk.vak12ed.edu) wrote:\n",
    ">>[...]\n",
    ">>A couple of years ago I put together a Tesla circuit which\n",
    ">>was published in an electronics magazine and could have been\n",
    ">>the circuit which is referred to here. This one used a\n",
    ">>flyback transformer from a tv onto which you wound your own\n",
    ">>primary windings. It also used 2 power transistors in a TO 3\n",
    ">[...]\n",
    ">10 years ago I built a 1'000,000 volt Tesla, and the thing was VERY\n",
    ">spectacular, but besides scaring/amazing friends (depending on their\n",
    ">knowledge of Science), and generating strong EMI, I never found anything\n",
    ">useful that could be done with it ...  Is there any real-world application\n",
    ">for Tesla coils today ?\n",
    ">\n",
    ">David Prutchi\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция для сплита письма на заголовок и тело письма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mail(text):\n",
    "    head_body = text.split(\"\\n\\n\")\n",
    "    return head_body[0], \"\\n\\n\".join(head_body[1:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример письма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: et@teal.csn.org (Eric H. Taylor)\n",
      "Subject: Re: HELP_WITH_TRACKING_DEVICE\n",
      "Summary: underground and underwater wireless methods\n",
      "Keywords: Rogers, Tesla, Hertz, underground, underwater, wireless, radio\n",
      "Nntp-Posting-Host: teal.csn.org\n",
      "Organization: 4-L Laboratories\n",
      "Expires: Fri, 30 Apr 1993 06:00:00 GMT\n",
      "Lines: 36\n",
      "\n",
      "In article <00969FBA.E640FF10@AESOP.RUTGERS.EDU> mcdonald@AESOP.RUTGERS.EDU writes:\n",
      ">[...]\n",
      ">There are a variety of water-proof housings I could use but the real meat\n",
      ">of the problem is the electronics...hence this posting.  What kind of\n",
      ">transmission would be reliable underwater, in murky or even night-time\n",
      ">conditions?  I'm not sure if sound is feasible given the distortion under-\n",
      ">water...obviously direction would have to be accurate but range could be\n",
      ">relatively short (I imagine 2 or 3  hundred yards would be more than enough)\n",
      ">\n",
      ">Jim McDonald\n",
      "\n",
      "Refer to patents by JAMES HARRIS ROGERS:\n",
      "958,829; 1,220,005; 1,322,622; 1,349,103; 1,315,862; 1,349,104;\n",
      "1,303,729; 1,303,730; 1,316,188\n",
      "\n",
      "He details methods of underground and underwater wireless communications.\n",
      "For a review, refer to _Electrical_Experimenter_, March 1919 and June 1919.\n",
      "\n",
      "Rogers' methods were used extensively during the World War, and was\n",
      "unclassified after the war. Supposedly, the government rethought this\n",
      "soon after, and Rogers was convieniently forgotten.\n",
      "\n",
      "The bottom line is that all antennas that are grounded send HALF of\n",
      "their signal THRU the ground. The half that travels thru space is\n",
      "quickly dissapated (by the square of the distance), but that which\n",
      "travels thru the ground does not disapate at all. Furthermore,\n",
      "the published data showed that when noise drowned out regular\n",
      "reception, the underground antennas would recieve virtually noise-free.\n",
      "\n",
      "IF you find this hard to believe, then refer to the work of the\n",
      "man who INVENTED wireless: Tesla. Tesla confirmed that Rogers' methods\n",
      "were correct, while Hertzian wave theory was completely \"abberant\".\n",
      "\n",
      "----\n",
      " ET   \"Tesla was 100 years ahead of his time. Perhaps now his time comes.\"\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mail = open(\"20news-bydate-train/sci.electronics/52434\", \"r\").read()\n",
    "print(mail)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат сплита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "head, body = split_mail(mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: et@teal.csn.org (Eric H. Taylor)\\nSubject: Re: HELP_WITH_TRACKING_DEVICE\\nSummary: underground and underwater wireless methods\\nKeywords: Rogers, Tesla, Hertz, underground, underwater, wireless, radio\\nNntp-Posting-Host: teal.csn.org\\nOrganization: 4-L Laboratories\\nExpires: Fri, 30 Apr 1993 06:00:00 GMT\\nLines: 36'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In article <00969FBA.E640FF10@AESOP.RUTGERS.EDU> mcdonald@AESOP.RUTGERS.EDU writes:\\n>[...]\\n>There are a variety of water-proof housings I could use but the real meat\\n>of the problem is the electronics...hence this posting.  What kind of\\n>transmission would be reliable underwater, in murky or even night-time\\n>conditions?  I\\'m not sure if sound is feasible given the distortion under-\\n>water...obviously direction would have to be accurate but range could be\\n>relatively short (I imagine 2 or 3  hundred yards would be more than enough)\\n>\\n>Jim McDonald\\n\\nRefer to patents by JAMES HARRIS ROGERS:\\n958,829; 1,220,005; 1,322,622; 1,349,103; 1,315,862; 1,349,104;\\n1,303,729; 1,303,730; 1,316,188\\n\\nHe details methods of underground and underwater wireless communications.\\nFor a review, refer to _Electrical_Experimenter_, March 1919 and June 1919.\\n\\nRogers\\' methods were used extensively during the World War, and was\\nunclassified after the war. Supposedly, the government rethought this\\nsoon after, and Rogers was convieniently forgotten.\\n\\nThe bottom line is that all antennas that are grounded send HALF of\\ntheir signal THRU the ground. The half that travels thru space is\\nquickly dissapated (by the square of the distance), but that which\\ntravels thru the ground does not disapate at all. Furthermore,\\nthe published data showed that when noise drowned out regular\\nreception, the underground antennas would recieve virtually noise-free.\\n\\nIF you find this hard to believe, then refer to the work of the\\nman who INVENTED wireless: Tesla. Tesla confirmed that Rogers\\' methods\\nwere correct, while Hertzian wave theory was completely \"abberant\".\\n\\n----\\n ET   \"Tesla was 100 years ahead of his time. Perhaps now his time comes.\"\\n----\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In article <00969FBA.E640FF10@AESOP.RUTGERS.EDU> mcdonald@AESOP.RUTGERS.EDU writes: [...] There are a variety of water-proof housings I could use but the real meat of the problem is the electronics...hence this posting.',\n",
       " 'What kind of transmission would be reliable underwater, in murky or even night-time conditions?',\n",
       " \"I'm not sure if sound is feasible given the distortion under- water...obviously direction would have to be accurate but range could be relatively short (I imagine 2 or 3  hundred yards would be more than enough)  Jim McDonald  Refer to patents by JAMES HARRIS ROGERS: 958,829; 1,220,005; 1,322,622; 1,349,103; 1,315,862; 1,349,104; 1,303,729; 1,303,730; 1,316,188  He details methods of underground and underwater wireless communications.\",\n",
       " 'For a review, refer to _Electrical_Experimenter_, March 1919 and June 1919.',\n",
       " \"Rogers' methods were used extensively during the World War, and was unclassified after the war.\",\n",
       " 'Supposedly, the government rethought this soon after, and Rogers was convieniently forgotten.',\n",
       " 'The bottom line is that all antennas that are grounded send HALF of their signal THRU the ground.',\n",
       " 'The half that travels thru space is quickly dissapated (by the square of the distance), but that which travels thru the ground does not disapate at all.',\n",
       " 'Furthermore, the published data showed that when noise drowned out regular reception, the underground antennas would recieve virtually noise-free.',\n",
       " 'IF you find this hard to believe, then refer to the work of the man who INVENTED wireless: Tesla.',\n",
       " 'Tesla confirmed that Rogers\\' methods were correct, while Hertzian wave theory was completely \"abberant\".',\n",
       " '----  ET   \"Tesla was 100 years ahead of his time.',\n",
       " 'Perhaps now his time comes\".']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = split_into_sentences(body)\n",
    "# for item in items:\n",
    "#     print(item)\n",
    "\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бьём на токены\n",
    "\n",
    "`tokenize` принимает на вход предложение и внутри него ищет токены.\n",
    "\n",
    "Снова Сначала ищем email и т п, реплейсим все точки запятые на `<dot>` `<coma>`\n",
    "\n",
    "\n",
    "После того как их зареплейсили, можно бить по точкам, запятым и прочему мусору.\n",
    "\n",
    "После того как побили токен, внутри него реплейсим `<dot>` обратно на точку, `<coma> обратно на запятую`\n",
    "\n",
    "В конце функции считаем стемму по токену, по стемме считаем лемму\n",
    "\n",
    "Интеракт функция - слайдер чтобы можно было листать предложения и смотреть результат разбиения в интеративе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02efc03bae004a24afa04925cc6e7858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=103), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from re import S\n",
    "\n",
    "\n",
    "files = os.listdir(\"20news-bydate-train/alt.atheism/\")\n",
    "file = 0\n",
    "print(files[file])\n",
    "mail = open(\"20news-bydate-train/alt.atheism/\" + files[file], \"r\").read()\n",
    "head, body = split_mail(mail)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    #TODO: Можно еще смайлики добавить но мне влом их все перегонять в словарь\n",
    "    #TODO: Можно еще добавить время 22:11\n",
    "    sentence = re.sub(f\">{word}\", \"><split>\\\\1\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"\\\\({word}\", \"(<split>\\\\1\", sentence)\n",
    "    sentence = re.sub(f\"{word}\\\\)\", \"\\\\1<split>)\", sentence)\n",
    "    sentence = re.sub(f\"I'm\", \"I<split>am\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"${word}'d\", \"\\\\1<split>woud\", sentence)\n",
    "    sentence = re.sub(f\"{number},{number}\", \"\\\\1<coma>\\\\2\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{number},{number},{number}\", \"\\\\1<coma>\\\\2<coma>\\\\3\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{number}'{number},{number}\", \"\\\\1'\\\\2<coma>\\\\3\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{number}[.]{number}\", \"\\\\1<dot>\\\\2\", sentence)\n",
    "    sentence = re.sub(f\"{number}[.]{number}.{number}\", \"\\\\1<dot>\\\\2<dot>\\\\3\", sentence)\n",
    "\n",
    "    sentence = re.sub(f\"{word}[.]{word}[.]{word}@{word}[.]{word}[.]{word}\", \"<split>\\\\1<dot>\\\\2<dot>\\\\3@\\\\4<dot>\\\\5<dot>\\\\6<split>\", sentence)\n",
    "    sentence = re.sub(f\"<{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<split>\\\\1<dot>\\\\2@\\\\3<dot>\\\\4<dot>\\\\5<split>\", sentence)\n",
    "    sentence = re.sub(f\"{word}@{word}[.]{word}[.]{word}\", \"\\\\1@\\\\2<dot>\\\\3<dot>\\\\4<split>\", sentence)\n",
    "    sentence = re.sub(f\"<{word}@{word}[.]{word}>\", \"<<split>\\\\1@\\\\2<dot>\\\\3<split>><split>\", sentence)\n",
    "    sentence = re.sub(f\"{word}@{word}[.]{word}\", \"\\\\1@\\\\2<dot>\\\\3<split>\", sentence)\n",
    "    sentence = re.sub(f\"<{word}[.]{word}@{word}[.]{word}[.]{word}>\", \"<split>\\\\1<dot>\\\\2@\\\\3<dot>\\\\4<dot>\\\\5<split>\", sentence)\n",
    "\n",
    "    sentence = sentence.replace(\"...\", '<dot><dot><dot>')\n",
    "    for r in \" \":\n",
    "        sentence = sentence.replace(r, '<split>')\n",
    "    for r in [\".\", \",\",\":\",\";\",\"?\",\"!\", '\"', \"'\", \"/\", \"*\", \"$\"]:\n",
    "        sentence = re.sub(f\"\\\\{r}{word}\", f\"{r}<split>\\\\1\", sentence)\n",
    "        sentence = re.sub(f\"{word}\\\\{r}\", f\"\\\\1<split>{r}\", sentence)\n",
    "\n",
    "    sentence = sentence.replace(\"<dot>\", \".\")\n",
    "\n",
    "    sentence = sentence.replace(\"<coma>\", \",\")\n",
    "\n",
    "    tokens = sentence.split('<split>')\n",
    "    return [(x, stemmer.stem(x), lemmatizer.lemmatize(stemmer.stem(x))) for x in tokens if x]\n",
    "\n",
    "maxI = len(split_into_sentences(body)) - 1\n",
    "\n",
    "@interact\n",
    "def test(i=widgets.IntSlider(min=0,max=maxI,step=1,value=0)):\n",
    "    sentence = split_into_sentences(body)[i]\n",
    "    pprint(sentence)\n",
    "    pprint(tokenize(sentence))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бьём на токены все файлы в директории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "folders = os.listdir(\"20news-bydate-train/\")\n",
    "print(folders)\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(f\"20news-bydate-train/{folder}/\")\n",
    "    \n",
    "    print(folder)\n",
    "    \n",
    "    for file in files:\n",
    "        if not os.path.exists(f\"out/train/{folder}\"):\n",
    "            os.makedirs(f\"out/train/{folder}\")\n",
    "        out = open(f\"out/train/{folder}/{file}.tsv\", \"w\")\n",
    "\n",
    "        mail = open(f\"20news-bydate-train/{folder}/{file}\", \"r\").read()\n",
    "        head, body = split_mail(mail)\n",
    "\n",
    "        for sentence in split_into_sentences(head):\n",
    "            for token, stem, lem in tokenize(sentence):\n",
    "                out.write(f\"{token}\\t{stem}\\t{lem}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "        for sentence in split_into_sentences(body):\n",
    "            for token, stem, lem in tokenize(sentence):\n",
    "                out.write(f\"{token}\\t{stem}\\t{lem}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "folders = os.listdir(\"20news-bydate-train/\")\n",
    "print(folders)\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(f\"20news-bydate-train/{folder}/\")\n",
    "    \n",
    "    print(folder)\n",
    "    out = open(f\"out/train/{folder}.tsv\", \"w\")\n",
    "    for file in files:\n",
    "        if not os.path.exists(f\"out/train/{folder}\"):\n",
    "            os.makedirs(f\"out/train/{folder}\")\n",
    "        \n",
    "\n",
    "        mail = open(f\"20news-bydate-train/{folder}/{file}\", \"r\").read()\n",
    "        head, body = split_mail(mail)\n",
    "\n",
    "        for sentence in split_into_sentences(head):\n",
    "            for token, stem, lem in tokenize(sentence):\n",
    "                out.write(f\"{token}\\t{stem}\\t{lem}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "        for sentence in split_into_sentences(body):\n",
    "            for token, stem, lem in tokenize(sentence):\n",
    "                out.write(f\"{token}\\t{stem}\\t{lem}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "    out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "239a9fa0287fc0fb48e1b84671738ced5172aadbf780861bac62c840610302f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
