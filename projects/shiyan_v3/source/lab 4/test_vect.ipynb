{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqsyj8kde7q_"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec.load(\"data\\\\w2v_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPadMjKgfAGG"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h3jrk0mfAus"
      },
      "outputs": [],
      "source": [
        "def SplitTextIntoSentences(txt):\n",
        "    dtxt=re.sub(r\"\\.\\s|\\.\\n\",\".天官赐福,百无禁忌%@!^&ЁЪЫї\",txt)\n",
        "    etxt=re.sub(r\"\\!\\s|\\!\\n\",\"!天官赐福,百无禁忌%@!^&ЁЪЫї\",dtxt)\n",
        "    qtxt=re.sub(r\"\\?\\s|\\?\\n\",\"?天官赐福,百无禁忌%@!^&ЁЪЫї\",etxt)\n",
        "    sentences= re.split(r'\\天\\官\\赐\\福\\,\\百\\无\\禁\\忌\\%\\@\\!\\^\\&\\Ё\\Ъ\\Ы\\ї', qtxt)\n",
        "\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYGwxKAsfE4N"
      },
      "outputs": [],
      "source": [
        "email = '[a-zA-Z0-9]+@\\w+[\\.\\w+]+'\n",
        "money='\\$[0-9]+[\\.[0-9]+]?[k]?'\n",
        "date='[0-9]{1,2}\\s[a-zA-Z]+\\s[0-9]{1,4}|[0-9]{1,2}\\s[0-9]{1,2}\\s[0-9]{1,4}|[0-9]{1,2}\\S[0-9]{1,2}\\S[0-9]{1,4}|[a-zA-Z]+\\s[0-9]{1,2}\\S?\\s[0-9]{1,4}'\n",
        "phone='\\([0-9]{3}\\)\\s[0-9]{3}\\-[0-9]{4}'\n",
        "words='[a-zA-z]+[\\'[a-z]+]?|\\S'\n",
        "prop_names=''\n",
        "full_exp=email+'|'+money+'|'+date+'|'+phone+'|'+words\n",
        "\n",
        "\n",
        "\n",
        "tokenize_regex = re.compile(full_exp, re.I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Dsum6tGfFhY"
      },
      "outputs": [],
      "source": [
        "def w2vec_vectorize(text: str, model):\n",
        "    sentence_list = SplitTextIntoSentences(text)\n",
        "    sent_vectors = []\n",
        "    for sentence in sentence_list:\n",
        "        tokens=tokenize_regex.findall(sentence)\n",
        "        word_vectors = []\n",
        "        for token in tokens:\n",
        "            if token not in stops:\n",
        "                try:\n",
        "                    word_vectors.append(model.wv[token.lower()])\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "\n",
        "        sent_vector = np.zeros(model.vector_size)\n",
        "        if (len(word_vectors) > 0):\n",
        "            sent_vector = (np.array([sum(x) for x in zip(*word_vectors)])) / sent_vector.size\n",
        "        sent_vectors.append(sent_vector)\n",
        "\n",
        "    vector = np.zeros(model.vector_size)\n",
        "    if (len(sent_vectors) > 0):\n",
        "        vector = (np.array([sum(x) for x in zip(*sent_vectors)])) / vector.size\n",
        "\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NILCsKANfJlc"
      },
      "outputs": [],
      "source": [
        "test_path=''\n",
        "result = ''\n",
        "\n",
        "count = 0\n",
        "for category in os.listdir(test_path):\n",
        "    for filename in os.listdir(test_path + category):\n",
        "        print(filename, count)\n",
        "        count += 1\n",
        "        with open(test_path + category + '/' + filename) as file:\n",
        "            vector = w2vec_vectorize(file.read(), model)\n",
        "            embedding_str = \"\"\n",
        "            for embedding in vector:\n",
        "                embedding_str += '\\t' + str(round(embedding, 6))\n",
        "            result += (category + '/' + filename + embedding_str + '\\n')\n",
        "\n",
        "with open('data\\\\test-embeddings.tsv', 'w') as result_file:\n",
        "    result_file.write(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
