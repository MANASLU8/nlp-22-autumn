{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разработка метода, позволяющего преобразовать произвольный текст в матрицу значений tf-idf (каждой строке обеих матриц соответствует одно предложение текста, каждому столбцу - один токен предложения), с использованием словаря наиболее частых слов и матрицы \"термин-документ\", полученных ранее. Так, при формировании матрицы значений tf-idf необходимо оценивать количество документов обучающей выборки, содержащих тот или иной термин (токен), а не количество предложений. Предложить и реализовать решение проблемы неодинакового количества токенов в предложении (наиболее простой вариант - заполнить \"лишние\" ячейки матрицы нулями). Реализовать способ подсчета векторного представления документа на основе векторных представлений отдельных предложений (наиболее простой вариант - подсчитать среднее значение по столбцам матрицы)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tfidf=t_{f}\\cdot log(\\frac{N_{d}}{d_{f}}),\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "t_{f}-term_{frequency}\n",
    "\\\\\n",
    "N_{d}-N_{document}\n",
    "\\\\\n",
    "d_{f}-document_{frequency}\n",
    "\\\\\n",
    "\\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data\\\\dictionary.pkl', \"rb\") as fp:\n",
    "        dictionary = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix=dict()\n",
    "\n",
    "with open('data\\\\td_matrix.pkl', \"rb\") as fp:\n",
    "        td_matrix = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix=dict()\n",
    "\n",
    "with open('data\\\\tf_matrix.pkl', \"rb\") as fp:\n",
    "        tf_matrix = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_in_d=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TokensByDoc(rootDir):\n",
    "    for lists in os.listdir(rootDir):\n",
    "                    path = os.path.join(rootDir, lists)\n",
    "                    for filename in glob.glob(path+'\\\\*'):\n",
    "                        with open(os.path.join(os.getcwd(), filename), 'r') as file:\n",
    "                            tokens=[]\n",
    "                            for line in file:\n",
    "                                if line != '\\n':\n",
    "                                    token = re.split(r'\\t', line)[0]\n",
    "                                    if not re.match('\\W|\\d',token) and token not in stops:\n",
    "                                        tokens.append(token)\n",
    "                            tok_in_d[os.path.splitext(os.path.basename(filename))[0]]= tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokensByDoc('D:\\\\S\\\\NLP\\\\nlp-22-autumn\\\\projects\\\\shiyan_v3\\\\assets\\\\annotated-corpus\\\\train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_in_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict(path,t):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(t, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict('data\\\\tok_in_d.pkl',tok_in_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data\\\\tok_in_d.pkl', \"rb\") as fp:\n",
    "        tok_in_d = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for token in dictionary:\n",
    "    for doc in tok_in_d.keys():\n",
    "        if token in tok_in_d[doc]:\n",
    "            td[token]=td.get(token,0)+1\n",
    "    count+=1\n",
    "    print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "N = len(tf_matrix) \n",
    "\n",
    "for sentence in tf_matrix.keys():\n",
    "    tokens=dict()\n",
    "    for token in tf_matrix[sentence].key():\n",
    "        tokens[token]=tf_matrix[sentence][token]* log10((N + 1)/(td[token] + 1))\n",
    "    tf_idf_matrix[sentence]=tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
