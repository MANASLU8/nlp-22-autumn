{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zmWWmxPI4LSF"},"outputs":[],"source":["import pickle\n","import os\n","import re\n","import nltk\n","import glob\n","import pandas as pd\n","import numpy as np\n","\n","from nltk.corpus import stopwords\n","nltk.download('stopwords', quiet=True)\n","\n","stops = set(stopwords.words(\"english\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxEm7DR10AYY"},"outputs":[],"source":["def SplitTextIntoSentences(txt):\n","    dtxt=re.sub(r\"\\.\\s|\\.\\n\",\".天官赐福,百无禁忌%@!^&ЁЪЫї\",txt)\n","    etxt=re.sub(r\"\\!\\s|\\!\\n\",\"!天官赐福,百无禁忌%@!^&ЁЪЫї\",dtxt)\n","    qtxt=re.sub(r\"\\?\\s|\\?\\n\",\"?天官赐福,百无禁忌%@!^&ЁЪЫї\",etxt)\n","    sentences= re.split(r'\\天\\官\\赐\\福\\,\\百\\无\\禁\\忌\\%\\@\\!\\^\\&\\Ё\\Ъ\\Ы\\ї', qtxt)\n","\n","    return sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhDw0xgl4Xd2"},"outputs":[],"source":["email = '[a-zA-Z0-9]+@\\w+[\\.\\w+]+'\n","money='\\$[0-9]+[\\.[0-9]+]?[k]?'\n","date='[0-9]{1,2}\\s[a-zA-Z]+\\s[0-9]{1,4}|[0-9]{1,2}\\s[0-9]{1,2}\\s[0-9]{1,4}|[0-9]{1,2}\\S[0-9]{1,2}\\S[0-9]{1,4}|[a-zA-Z]+\\s[0-9]{1,2}\\S?\\s[0-9]{1,4}'\n","phone='\\([0-9]{3}\\)\\s[0-9]{3}\\-[0-9]{4}'\n","words='[a-zA-z]+[\\'[a-z]+]?|\\S'\n","prop_names=''\n","full_exp=email+'|'+money+'|'+date+'|'+phone+'|'+words\n","\n","\n","\n","tokenize_regex = re.compile(full_exp, re.I)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVEWjSA-5nnh"},"outputs":[],"source":["dictionary=dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGX2dQ6F5onZ"},"outputs":[],"source":["with open('data\\\\dictionary.pkl', \"rb\") as fp:\n","        dictionary = pickle.load(fp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9AMF56X0AYd"},"outputs":[],"source":["def w2vec_vectorize(text: str, model):\n","    sentence_list = SplitTextIntoSentences(text)\n","    sent_vectors = []\n","    for sentence in sentence_list:\n","        tokens=tokenize_regex.findall(sentence)\n","        word_vectors = []\n","        for token in tokens:\n","            if token not in stops:\n","                try:\n","                    word_vectors.append(model.wv[token.lower()])\n","                except Exception as e:\n","                    print(e)\n","\n","        sent_vector = np.zeros(model.vector_size)\n","        if (len(word_vectors) > 0):\n","            sent_vector = (np.array([sum(x) for x in zip(*word_vectors)])) / sent_vector.size\n","        sent_vectors.append(sent_vector)\n","\n","    vector = np.zeros(model.vector_size)\n","    if (len(sent_vectors) > 0):\n","        vector = (np.array([sum(x) for x in zip(*sent_vectors)])) / vector.size\n","\n","    return vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oF1LNW_P_veh"},"outputs":[],"source":["def sentence_list(rootDir):\n","    sentence_list = [] \n","    for lists in os.listdir(rootDir):\n","      path = os.path.join(rootDir, lists)\n","      for filename in glob.glob(path+'/*'):\n","        with open(os.path.join(os.getcwd(), filename), 'r',errors='ignore') as file:\n","          sentences=SplitTextIntoSentences(file.read().rstrip())\n","          for s in sentences:\n","            tokens=tokenize_regex.findall(s)\n","            tokens_filtred = [t for t in tokens if not re.match(r'\\W|\\d',t) and t not in stops]\n","            sentence_list.append(tokens_filtred)\n","    #print(sentence_list)\n","    with open(\"data\\\\sentence_list\", \"wb\") as file:\n","        pickle.dump(sentence_list, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhwGuqpeC8GZ"},"outputs":[],"source":["sentence_list(\"D:\\\\S\\\\NLP\\\\nlp-22-autumn\\\\projects\\\\shiyan_v3\\\\assets\\\\data\\\\train\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLN5fDDiDUW-"},"outputs":[],"source":["with open(\"data\\\\sentence_list\", \"rb\") as file:\n","     sentences = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1679827815226,"user":{"displayName":"Анна Сергеевна Шиян","userId":"10567142615714490565"},"user_tz":-180},"id":"z9m_V7jtDkix","outputId":"56a56db6-3aa9-4cb7-e81e-5eb42a5e2adb"},"outputs":[],"source":["sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWPWTFz-0AYo"},"outputs":[],"source":["import gensim.models\n","\n","w2v =  gensim.models.Word2Vec(sentences=sentences, window=10, vector_size=100, epochs=5)\n","\n","w2v.save('data\\\\w2v_model')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
