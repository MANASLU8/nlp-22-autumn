{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание директории для данных с опечатками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import os\n",
    "\n",
    "def AddDir(rootDir):\n",
    "        for lists in os.listdir(rootDir):\n",
    "                path = os.path.join(rootDir, lists)\n",
    "                new_dir = re.sub(\"corrupted_date\",\"annotated-corpus-corrupted\",path)\n",
    "                os.mkdir(new_dir)\n",
    "                print(new_dir)\n",
    "AddDir('D:\\\\S\\\\NLP\\\\nlp-22-autumn\\\\projects\\\\shiyan_v3\\\\assets\\\\corrupted_date\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация данных с опечатками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import Token\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re\n",
    "import os\n",
    "import glob\n",
    "\n",
    "email = '[a-zA-Z0-9]+@\\w+[\\.\\w+]+'\n",
    "money='\\$[0-9]+[\\.[0-9]+]?[k]?'\n",
    "date='[0-9]{1,2}\\s[a-zA-Z]+\\s[0-9]{1,4}|[0-9]{1,2}\\s[0-9]{1,2}\\s[0-9]{1,4}|[0-9]{1,2}\\S[0-9]{1,2}\\S[0-9]{1,4}|[a-zA-Z]+\\s[0-9]{1,2}\\S?\\s[0-9]{1,4}'\n",
    "phone='\\([0-9]{3}\\)\\s[0-9]{3}\\-[0-9]{4}'\n",
    "words='[a-zA-z]+[\\'[a-z]+]?|w+|\\S'\n",
    "prop_names=''\n",
    "full_exp=email+'|'+money+'|'+date+'|'+phone+'|'+words\n",
    "\n",
    "path_change='D:\\\\S\\\\NLP\\\\nlp-22-autumn\\\\projects\\\\shiyan_v3\\\\assets\\\\annotated-corpus-corrupted\\\\'\n",
    "\n",
    "ps =PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def DataProcess(rootDir):\n",
    "    for lists in os.listdir(rootDir):\n",
    "                    path = os.path.join(rootDir, lists)\n",
    "                    for filename in glob.glob(path+'\\\\*'):\n",
    "                        with open(os.path.join(os.getcwd(), filename), 'r') as file:\n",
    "                            \n",
    "                            txt = file.read().rstrip()\n",
    "\n",
    "                            dtxt=re.sub(r\"\\.\\s|\\.\\n\",\".天官赐福,百无禁忌%@!^&ЁЪЫї\",txt)\n",
    "                            etxt=re.sub(r\"\\!\\s|\\!\\n\",\"!天官赐福,百无禁忌%@!^&ЁЪЫї\",dtxt)\n",
    "                            qtxt=re.sub(r\"\\?\\s|\\?\\n\",\"?天官赐福,百无禁忌%@!^&ЁЪЫї\",etxt)\n",
    "                            sentences= re.split(r'\\天\\官\\赐\\福\\,\\百\\无\\禁\\忌\\%\\@\\!\\^\\&\\Ё\\Ъ\\Ы\\ї', qtxt)\n",
    "\n",
    "                            tokenize_regex = re.compile(full_exp, re.I)\n",
    "                            new_path = re.sub(\"corrupted_date\",\"annotated-corpus-corrupted\",file.name)\n",
    "\n",
    "                            with open(new_path+\".tsv\", \"w\") as f:\n",
    "                                for s in sentences:\n",
    "                                    tokens =tokenize_regex.findall(s)\n",
    "                                    for t in tokens:\n",
    "                                        f.write(t)\n",
    "                                        f.write('\\t')\n",
    "                                        f.write(ps.stem(t))\n",
    "                                        f.write('\\t')\n",
    "                                        f.write(lemmatizer.lemmatize(t))\n",
    "                                        f.write('\\n')\n",
    "                                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcess('D:\\\\S\\\\NLP\\\\nlp-22-autumn\\\\projects\\\\shiyan_v3\\\\assets\\\\corrupted_date\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_file_tokens(path):\n",
    "    tokens = []\n",
    "    with open(path, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            if line:\n",
    "                token = line.split(\"\\t\")[0]\n",
    "                if token != '\\n':\n",
    "                    tokens.append(token)\n",
    "            line = f.readline()\n",
    "    f.close()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def GetAllTokens(rootDir):\n",
    "    tokens=[]\n",
    "    for lists in os.listdir(rootDir):\n",
    "                    path = os.path.join(rootDir, lists)\n",
    "                    for filename in glob.glob(path+'\\\\*'):\n",
    "                       tokens = tokens + get_file_tokens(filename)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = GetAllTokens('D:\\\\S\\\\NLP\\\\nlp-22-autumn\\\\projects\\\\shiyan_v3\\\\assets\\\\annotated-corpus\\\\test\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict(path,t):\n",
    "    with open(path, \"w\") as f:\n",
    "        for i in t:\n",
    "            f.write(i)\n",
    "            f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "dict = list(OrderedDict.fromkeys(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict(\"dictionary.csv\", dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict(path):\n",
    "    dictionary = []\n",
    "    with open(path, \"r\") as f:\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            dictionary.append(line.replace('\\n',''))\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=read_dict(\"dictionary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поиск опечаток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "keyboard_array_mapping = [\n",
    "    ['`~', '1!', '2@', '3#', '4$', '5%', '6^', '7&', '8*', '9(', '0)', '-_', '=+'],\n",
    "    ['qQ', 'wW', 'eE', 'rR', 'tT', 'yY', 'uU', 'iI', 'oO', 'pP', '[{', ']}', '\\\\|'],\n",
    "    ['aA', 'sS', 'dD', 'fF', 'gG', 'hH', 'jJ', 'kK', 'lL', ';:', '\\'\"'],\n",
    "    ['zZ', 'xX', 'cC', 'vV', 'bB', 'nN', 'mM', ',<', '.>', '/?'],\n",
    "    [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
    "]\n",
    "\n",
    "\n",
    "def is_symbol_in_keyboard_array(symbol, array):\n",
    "    return True in [symbol in row for row in array]\n",
    "\n",
    "\n",
    "def get_keyboard_symbol_location(symbol, array):\n",
    "    for array_row in array:\n",
    "        if symbol in array_row:\n",
    "            row = array.index(array_row)\n",
    "            column = array_row.index(symbol)\n",
    "            return row, column\n",
    "    raise ValueError(\"{} not found\".format(symbol))\n",
    "\n",
    "\n",
    "def get_keyboard_distance(symbol1, symbol2):\n",
    "    coord1 = get_keyboard_symbol_location(symbol1, keyboard_array_mapping)\n",
    "    coord2 = get_keyboard_symbol_location(symbol2, keyboard_array_mapping)\n",
    "    return np.sqrt(np.square(coord1[0] - coord2[0]) + np.square(coord1[1] - coord2[1]))\n",
    "\n",
    "\n",
    "def get_max_keyboard_distance():\n",
    "    get_keyboard_distance(keyboard_array_mapping[0][0], keyboard_array_mapping[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "removal_cost = 1\n",
    "insertion_cost = 1\n",
    "substitution_cost = 2\n",
    "no_op_cost = 0\n",
    "\n",
    "\n",
    "def get_weighted_cost(replace_cost, symbol1, symbol2):\n",
    "    try:\n",
    "        return replace_cost + (get_keyboard_distance(symbol1, symbol2) / get_max_keyboard_distance())\n",
    "    except ValueError:\n",
    "        return substitution_cost\n",
    "\n",
    "\n",
    "def count_edit_distance(word1, word2):\n",
    "    n = len(word1) + 1\n",
    "    m = len(word2) + 1\n",
    "    matrix = np.zeros((n, m))\n",
    "    for i in range(1, n):\n",
    "        matrix[i][0] = i * removal_cost\n",
    "    for j in range(1, m):\n",
    "        matrix[0][j] = j * insertion_cost\n",
    "    for i in range(1, n):\n",
    "        for j in range(1, m):\n",
    "            matrix[i, j] = min(\n",
    "                matrix[i - 1][j] + removal_cost,\n",
    "                matrix[i][j - 1] + insertion_cost,\n",
    "                matrix[i - 1][j - 1] + (get_weighted_cost(\n",
    "                    substitution_cost - 1,\n",
    "                    word1[i - 1],\n",
    "                    word2[j - 1])\n",
    "                )\n",
    "                if word1[i - 1] != word2[j - 1] else no_op_cost)\n",
    "    return matrix[n - 1, m - 1]\n",
    "\n",
    "\n",
    "def fix_token_typo(token, tokens_dictionary_list):\n",
    "    if token in tokens_dictionary_list:\n",
    "        return token\n",
    "    min_distance = 10000\n",
    "    fixed_token = \"\"\n",
    "    for correct_token in tokens_dictionary_list:\n",
    "        distance = count_edit_distance(token, correct_token)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            fixed_token = correct_token\n",
    "    return fixed_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "\n",
    "def print_score(valid_tokens_in_corrupted_file_cnt,\n",
    "                correct_tokens_in_corrupted_file_before_cnt,\n",
    "                correct_tokens_in_corrupted_file_after_cnt):\n",
    "    print(\"Общее количество токенов: {}\".format(valid_tokens_in_corrupted_file_cnt))\n",
    "    print(\"Количество не содержащее опечатки до обработки: {}\".format(correct_tokens_in_corrupted_file_before_cnt))\n",
    "    print(\"Количество не содержащее опечатки после обработки: {}\".format(correct_tokens_in_corrupted_file_after_cnt))\n",
    "    print(\"До обработки: {}\"\n",
    "          .format(correct_tokens_in_corrupted_file_before_cnt / valid_tokens_in_corrupted_file_cnt))\n",
    "    print(\"После: {}\"\n",
    "          .format(correct_tokens_in_corrupted_file_after_cnt / valid_tokens_in_corrupted_file_cnt))\n",
    "\n",
    "\n",
    "def handle_corrupted_file(path_to_corrupted_tokens_dataset, path_to_correct_tokens_dataset):\n",
    "    valid_tokens_in_corrupted_file_cnt = 0\n",
    "    correct_tokens_in_corrupted_file_before_cnt = 0\n",
    "    correct_tokens_in_corrupted_file_after_cnt = 0\n",
    "\n",
    "    token_dict = dictionary\n",
    "    fixed_token_cnt = 1\n",
    "    token_counter_limit = 200\n",
    "    for root, dirs, files in os.walk(path_to_corrupted_tokens_dataset):\n",
    "        if fixed_token_cnt == token_counter_limit:\n",
    "            break\n",
    "        for file in files:\n",
    "            if fixed_token_cnt == token_counter_limit:\n",
    "                break\n",
    "            corrupted_token_list = get_file_tokens(root + '\\\\' + file)\n",
    "            corrupted_token_list_cnt = len(corrupted_token_list)\n",
    "            original_token_list = get_file_tokens(\n",
    "                root.replace(path_to_corrupted_tokens_dataset, path_to_correct_tokens_dataset+'\\\\') + '\\\\' + file)\n",
    "            original_token_list_cnt = len(original_token_list)\n",
    "            if corrupted_token_list_cnt != original_token_list_cnt:\n",
    "                print(\"Skip: {}\".format(root + '/' + file))\n",
    "            else:\n",
    "                valid_tokens_in_corrupted_file_cnt += corrupted_token_list_cnt\n",
    "                for index, token in enumerate(corrupted_token_list):\n",
    "                    if fixed_token_cnt == token_counter_limit:\n",
    "                        break\n",
    "                    token = token.split()[0]\n",
    "                    fixed_token = fix_token_typo(token, token_dict)\n",
    "                    if fixed_token == token:\n",
    "                        correct_tokens_in_corrupted_file_before_cnt += 1\n",
    "                    else:\n",
    "                        corrupted_token_list[index] = fixed_token\n",
    "                    fixed_token_cnt += 1\n",
    "                for index, token in enumerate(corrupted_token_list):\n",
    "                    if corrupted_token_list[index] == original_token_list[index]:\n",
    "                        correct_tokens_in_corrupted_file_after_cnt += 1\n",
    "    print_score(\n",
    "        valid_tokens_in_corrupted_file_cnt,\n",
    "        correct_tokens_in_corrupted_file_before_cnt,\n",
    "        correct_tokens_in_corrupted_file_after_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip: D:\\S\\NLP\\nlp-22-autumn\\projects\\shiyan_v3\\assets\\annotated-corpus-corrupted\\alt.atheism/53068.tsv\n",
      "Skip: D:\\S\\NLP\\nlp-22-autumn\\projects\\shiyan_v3\\assets\\annotated-corpus-corrupted\\alt.atheism/53257.tsv\n",
      "Skip: D:\\S\\NLP\\nlp-22-autumn\\projects\\shiyan_v3\\assets\\annotated-corpus-corrupted\\alt.atheism/53260.tsv\n",
      "Общее количество токенов: 280\n",
      "Количество не содержащее опечатки до обработки: 176\n",
      "Количество не содержащее опечатки после обработки: 223\n",
      "До обработки: 0.6285714285714286\n",
      "После: 0.7964285714285714\n"
     ]
    }
   ],
   "source": [
    "handle_corrupted_file('D:\\\\S\\\\NLP\\\\nlp-22-autumn\\\\projects\\\\shiyan_v3\\\\assets\\\\annotated-corpus-corrupted\\\\','D:\\\\S\\\\NLP\\\\nlp-22-autumn\\\\projects\\\\shiyan_v3\\\\assets\\\\annotated-corpus\\\\test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07bf0638b6f9a97e679c0f68f4d26207adc48a6db7abe23794ff5c2169363bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
